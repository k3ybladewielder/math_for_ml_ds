{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f77ba1e",
   "metadata": {},
   "source": [
    "# Calculus for Machine Learning and Data Science\n",
    "\n",
    "Notas sobre o curso Calculus for Machine Learning and Data Science da DeeplearninigAI.\n",
    "\n",
    "Repositório com a trilha Mathematics for Machine Learninig and Data Science:\n",
    "https://github.com/k3ybladewielder/math_for_ml_ds\n",
    "\n",
    "Fórum do curso: https://community.deeplearning.ai/c/math-for-machine-learning/m4ml-course-2/304"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b35a866",
   "metadata": {},
   "source": [
    "# Week 1 - Derivatives and Optimization\n",
    "\n",
    "Objectives\n",
    "- Perform gradient descent in neural networks with different activation and cost functions\n",
    "- Visually interpret differentiation of different types of functions commonly used in machine learning\n",
    "- Approximately optimize different types of functions commonly used in machine learning using first-order (gradient descent) and second-order (Newton’s method) iterative methods\n",
    "- Analytically optimize different types of functions commonly used in machine learning using properties of derivatives and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a832af",
   "metadata": {},
   "source": [
    "## Concept of Derivatives and Tangents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974844e",
   "metadata": {},
   "source": [
    "A derivada é a taxa instantânea de variação de uma função. É uma medida da taxa de variação instantânea de uma função em relação à sua variável independente. Em outras palavras, a derivada de uma função f(x) em um ponto x é a inclinação da tangente à curva da função nesse ponto. A derivada de uma função num ponto é a inclinação (slope) da tangente nesse ponto. Geometricamente, pode ser entendida como a taxa de mudança (rate of change) da função em um determinado ponto. \n",
    "\n",
    "Exemplo: Imagine que você está andando de bicicleta em uma estrada montanhosa, onde a estrada é a representação da função matemática. A derivada é como a medida de quão rápido você está subindo ou descendo a colina em um ponto específico da estrada. Se você está subindo uma colina íngreme, você está indo devagar, então a derivada é pequena. Se você está descendo a colina rapidamente, você está indo mais rápido, então a derivada é grande.\n",
    "\n",
    "É uma \"força que aplica em cada entrada pra aumentar a saída.\", \"É uma forma inteligente de saber como deve alterar as entradas pra levemente aumentar a sua saída\"\n",
    "\n",
    "A tangente de uma derivada é a reta que toca suavemente uma curva em um ponto específico e representa a taxa de mudança instantânea daquela curva naquele ponto.\n",
    "\n",
    "Imagine que você está desenhando uma curva em um papel, e em algum ponto dessa curva você desenha uma reta que toca suavemente a curva. Essa reta é a tangente da curva naquele ponto, e a inclinação da reta representa a taxa de mudança instantânea da curva naquele ponto (derivada).\n",
    "\n",
    "A tangente de uma derivada é útil porque nos permite entender como uma curva está mudando em um ponto específico e, portanto, pode nos ajudar a prever como ela mudará em outros pontos próximos. A tangente da derivada também é usada em cálculo para encontrar o ponto máximo ou mínimo de uma função, bem como para calcular a velocidade, a aceleração e outras medidas importantes em física e engenharia\n",
    "\n",
    "Em machine learninig, derivada é usada para otimizar funções, pra maximizar ou minimizar um valor de uma função. Isso é feito pra encontrar os parâmetros do modelo que melhor se ajuste aos dados, a partir do cálculo e minimização da loss funcion. \n",
    "\n",
    "<img src=\"./imgs/derivative.png\">\n",
    "\n",
    "O valor máximo ou mínimo ocorre quando da função, acontece quando a derivada é zero. Quando a linha da tangente é horizontal.\n",
    "\n",
    "<img src=\"./imgs/maxima.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb30fb",
   "metadata": {},
   "source": [
    "## Derivatives and their notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db093c6",
   "metadata": {},
   "source": [
    "Sabemos que a inclinação (slope) é calculada como a variação do X sobre o Y, sendo $\\frac{\\Delta x}{\\Delta y}$. Assim, a inclunação num ponto específico é $\\frac{dx}{dt}$. No geral, usamos o X no eixo horizontal e Y no eixo vertical, logo, a derivada fica como $\\frac{dx}{dy}$\n",
    "\n",
    "Sendo sua função $$ y = f(x)$$ representamos a derivada usando a **notação de Lagrange** como:\n",
    "\n",
    "<center>$$f'(x)$$</center>\n",
    "\n",
    "\n",
    "Também podemos representar a derivada usando a **notação de Leibniz**:\n",
    "<center>$$ \\frac{dy}{dx} = \\frac{d}{dx}f(x)$$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f2ee94",
   "metadata": {},
   "source": [
    "## Some common Derivatives - Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8a2e52",
   "metadata": {},
   "source": [
    "Em retas/linhas, a tangente em todos os pontos é o mesmo. Por isso são as mais simples.\n",
    "\n",
    "Neste primeiro exemplo temos uma constante. Nela, a altura do $x0 = x1 = xn$, ou seja, em qualquer ponto o valor da altura é o mesmo, é constante. Assim como como a tangente, ela é a mesma em todos os pontos, possui a mesma inclunação em qualquer ponto. Aqui, a inclunação é **zero**. Constantes ou linhas horizontais sempre terá a derivada de **zero**\n",
    "\n",
    "<img src=\"./imgs/slope_line1.png\">\n",
    "\n",
    "Neste segundo exemplo, temos a função $f(x) = ax + b$, em que o a é a inclunação da reta e o b é o seu intercepto, ou seja, o ponto em que toca no eixo y. A derivada dessa função é o $\\frac{\\Delta y}{\\Delta x} = a$. Aqui a tangente possui a mesma inclunação em todos os pontos, assim como a constante. Sempre que temos uma reta, todos os pontos possuem a mesma inclinação. Sendo $f'(x) = a$ \n",
    "\n",
    "Imagine que você está andando de bicicleta em uma estrada reta. A sua velocidade é constante, o que significa que você está pedalando na mesma velocidade o tempo todo. Agora, se você começa a acelerar ou desacelerar, a sua velocidade muda, certo?\n",
    "\n",
    "A mesma coisa acontece com uma função matemática. Quando você tem uma função como $f(x) = ax + b$, ela representa uma linha reta no gráfico. Se a \"a\" é positiva, significa que a linha está inclinada para cima, e se a \"a\" é negativa, a linha está inclinada para baixo.\n",
    "\n",
    "A derivada da função f(x) **é a taxa de mudança da inclinação da linha**. Em outras palavras, é o **quanto a linha está inclinada em cada ponto**. Se a função é uma linha reta, a inclinação é sempre a mesma, então a derivada é sempre a mesma - a inclinação da linha, que é o \"a\".\n",
    "\n",
    "Então, em resumo, a derivada de f(x) = ax + b é sempre igual a \"a\", que é a inclinação da linha.\n",
    "\n",
    "<img src=\"./imgs/slope_line2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5b8e1",
   "metadata": {},
   "source": [
    "## Some common Derivatives - Quadratics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0536e9f",
   "metadata": {},
   "source": [
    "A função quadrática mais simples é uma parábola, com a equação $y = f(x) = x^{2}$. Aqui, à esquerda do eixo Y temos valores negativos e à direita temos valores positivos. A fórmula da sua derivada é $\\frac{\\Delta f}{\\Delta x}$ quando o $\\Delta x$ vai a zero. $\\Delta f$ é a variação em y ou a variação em f, \n",
    "\n",
    "$$\\frac{\\Delta f}{\\Delta x} = \\frac{(x + \\Delta x)^{2} - (x){2}}{\\Delta x} $$\n",
    "\n",
    "Quando x = 1,\n",
    "\n",
    "$$\\Delta f = (1 + 1){2} - 1^{2} = 4 - 1 = 3$$\n",
    "\n",
    "E o $$\\Delta x = 1$$\n",
    "\n",
    "Assim $$\\frac{\\Delta f}{\\Delta x} = \\frac{3}{1} = 3$$\n",
    "\n",
    "Quanto mais diminuímos o intervalo, mais diminuímos o $\\Delta f$, mais a variação em y diminui. Neste exemplo, quanto mais diminuímos, mais a inclunação se aproxima de 2, isso porquê a inclinação da tangente em 1 é 2. E 2 é 2 vezes 1.\n",
    "\n",
    "$$f'(1) = \\frac{d}{dx}f(1) = 2$$\n",
    "\n",
    "<img src=\"./imgs/slope_quadratic1.png\">\n",
    "\n",
    "Imagine que você tem uma bola e quer saber em que direção ela está indo. Se você olhar para a bola em dois momentos diferentes, você pode ver que ela está se movendo. A derivada é como se você olhasse para a bola em dois momentos e calculasse a velocidade dela.\n",
    "\n",
    "A mesma coisa acontece com a função quadrática f(x) = x². Se você desenha essa função no papel, ela forma uma curva. A derivada dessa função é como se você olhasse para a curva em dois pontos diferentes e calculasse a inclinação dela. E, para essa função, a inclinação muda a cada ponto ao longo da curva.\n",
    "\n",
    "Quando você calcula a derivada de f(x) = x², você obtém f(x) = 2x. Isso significa que a inclinação da curva em cada ponto é igual a 2 vezes o valor de x naquele ponto. Em outras palavras, quanto maior o valor de x, mais íngreme é a curva.\n",
    "\n",
    "Então, em resumo, a derivada da função quadrática f(x) = x² é f(x) = 2x, o que significa que a inclinação da curva muda a cada ponto ao longo da curva e é igual a 2 vezes o valor de x em cada ponto\n",
    "\n",
    "<img src=\"./imgs/slope_quadratic2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e72812b",
   "metadata": {},
   "source": [
    "## Some common Derivatives - Higher Degree Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f2aff",
   "metadata": {},
   "source": [
    "A derivada de uma função cúbica é bem parecida com a quadrada, e seu comportamento é semelhante. À medida que $\\Delta x$ vai para zero, esse valor se torna a inclinação da linha tangente ou a derivada. Sua função é \n",
    "\n",
    "$$y = f(x) = x^{3} $$\n",
    "\n",
    "E sua inclinação se dá por:\n",
    "\n",
    "$$\\frac{\\Delta f}{\\Delta x} = \\frac{(x + \\Delta x)^{3} - (x){3}}{\\Delta x} $$\n",
    "\n",
    "Vamos imaginar que você tem uma pilha de blocos que cresce a cada minuto. Se você quiser saber a velocidade com que a pilha está crescendo, você precisa olhar para a pilha em dois momentos diferentes e calcular a diferença entre elas. A derivada é como se você fizesse isso muitas vezes, em um intervalo de tempo muito pequeno, para saber a velocidade exata da pilha.\n",
    "\n",
    "A mesma coisa acontece com a função cúbica f(x) = x³. A derivada dessa função é como se você olhasse para a curva em dois pontos diferentes e calculasse a inclinação dela. E, para essa função, a inclinação também muda a cada ponto ao longo da curva.\n",
    "\n",
    "Quando você calcula a derivada de f(x) = x³, você obtém f(x) = 3x². Isso significa que a inclinação da curva em cada ponto é igual a 3 vezes o valor de x ao quadrado. Em outras palavras, **quanto maior o valor de x, mais íngreme é a curva**, e a **inclinação aumenta mais rapidamente quanto maior o valor de x**.\n",
    "\n",
    "Então, em resumo, a derivada da função cúbica f(x) = x³ é f(x) = 3x², o que significa que a inclinação da curva muda a cada ponto ao longo da curva e é igual a 3 vezes o valor de x ao quadrado em cada ponto.\n",
    "\n",
    "<img src=\"./imgs/slope_cubic1.png\">\n",
    "<img src=\"./imgs/slope_cubic2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebf66e0",
   "metadata": {},
   "source": [
    "## Some common Derivatives - Other Power Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ad8873",
   "metadata": {},
   "source": [
    "A derivada de uma hipérbota, ou 1/x é um pouco mais complicada, mas o cálculo é relativamente simples. \n",
    "\n",
    "Sua função se dá por\n",
    "\n",
    "$$ y = f(x) = x^{-1} = \\frac{1}{x}$$\n",
    "\n",
    "E sua inclinação\n",
    "\n",
    "$$\\frac{\\Delta f}{\\Delta x} = \\frac{(x + \\Delta x)^{-1} - (x){-1}}{\\Delta x} $$\n",
    "\n",
    "Imagine que você tem uma barra de chocolate e a divide em pedaços iguais. Se você tem 4 pedaços, cada um é um quarto da barra inteira. Se você tem 8 pedaços, cada um é um oitavo da barra inteira. Quanto mais pedaços você tem, menor é cada pedaço.\n",
    "\n",
    "A mesma ideia acontece com a função y = 1/x. Essa função representa uma curva que começa em um ponto muito alto no eixo y, perto do infinito positivo, e vai diminuindo de tamanho à medida que se aproxima do eixo x. Quanto mais perto da origem, menor é o valor da função.\n",
    "\n",
    "A derivada dessa função é como se você dividisse a curva em infinitos pedaços, de tamanho cada vez menor. A cada pedaço, você calcula a inclinação da curva nesse ponto específico. Para a função y = 1/x, a inclinação muda em cada ponto ao longo da curva.\n",
    "\n",
    "Quando você calcula a derivada de y = 1/x, você obtém y' = -1/x². Isso significa que a inclinação da curva em cada ponto é igual a -1 dividido pelo valor de x ao quadrado. Em outras palavras, quanto maior o valor de x, menor é a inclinação da curva. E, quando x se aproxima de zero, a inclinação da curva aumenta muito rapidamente, tornando-se cada vez mais íngreme.\n",
    "\n",
    "Então, em resumo, a derivada da função y = 1/x é y' = -1/x², o que significa que a inclinação da curva muda em cada ponto ao longo da curva e é igual a -1 dividido pelo valor de x ao quadrado em cada ponto.\n",
    "\n",
    "<img src=\"./imgs/slope_hip1.png\">\n",
    "\n",
    "<img src=\"./imgs/slope_hip2.png\">\n",
    "\n",
    "Agora que vimos várias derivadas de potencias, podemos identificar um padrão:\n",
    "- O expoente se torna o fator de multiplicação\n",
    "- O novo expoente é o anterior -1\n",
    "\n",
    "Da mesma forma, se tivermos a função $f(x) = x^{n}$ a derivada será $f'(x) = xn{x-1}$\n",
    "\n",
    "<img src=\"./imgs/slope_pattern.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a713c70",
   "metadata": {},
   "source": [
    "## The Inverse Function and its Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54f7170",
   "metadata": {},
   "source": [
    "Uma função sempre faz alguma coisa, e a função que desfaz isso é a **função inversa**. Por exemplo, se uma função transforma o valor de 3 em 5, então a função inversa vai tornar o valor de 5 em 3. A derivada de uma função inversa tem o mesmo sentido.\n",
    "\n",
    "<img src=\"./imgs/inverse_func.png\">\n",
    "\n",
    "A tangente de cada derivada também terá a mesma ligação, a tangente da derivada de uma função inversa é um reflexo da derivada de uma função correspondente.\n",
    "\n",
    "O g(y) é a função inversa, então a derivada de g é 1/f'(x)\n",
    "\n",
    "<img src=\"./imgs/inverse_function_slope1.png\">\n",
    "\n",
    "<img src=\"./imgs/inverse_function_slope2.png\">\n",
    "\n",
    "<img src=\"./imgs/inverse_function_slope3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7ef8a",
   "metadata": {},
   "source": [
    "## Derivative of Trigonometric Functions (rever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf6b8e",
   "metadata": {},
   "source": [
    "O seno, cosseno e tangente são funções matemáticas que estão relacionadas com ângulos em um triângulo retângulo.\n",
    "\n",
    "O seno é a razão entre o lado oposto ao ângulo e a hipotenusa do triângulo retângulo. De maneira mais simples, podemos pensar no seno como a altura de um ponto no triângulo em relação à sua base.\n",
    "\n",
    "O cosseno é a razão entre o lado adjacente ao ângulo e a hipotenusa do triângulo retângulo. Podemos pensar no cosseno como a largura de um ponto no triângulo em relação à sua altura.\n",
    "\n",
    "A tangente é a razão entre o lado oposto ao ângulo e o lado adjacente ao ângulo no triângulo retângulo. Podemos pensar na tangente como a inclinação de um ponto no triângulo em relação à base.\n",
    "\n",
    "As funções trigonométricas como o seno, cosseno e tangente são comumente utilizadas em machine learning para modelar relações não lineares em dados. Uma das maneiras de utilizar essas funções é através de suas derivadas.\n",
    "\n",
    "As derivadas das funções trigonométricas são úteis em machine learning para encontrar a inclinação ou a taxa de variação da função em um determinado ponto. Isso é especialmente útil em algoritmos de otimização, onde a tarefa é encontrar o mínimo ou máximo de uma função.\n",
    "\n",
    "Por exemplo, em redes neurais, as funções de ativação são frequentemente escolhidas para serem não lineares, a fim de permitir que o modelo capture relações complexas nos dados. As funções trigonométricas como o seno e o cosseno podem ser usadas como funções de ativação, como a função seno hiperbólico (tanh), que é uma variação da função seno. A derivada da função seno hiperbólico é facilmente computada a partir da derivada do seno.\n",
    "\n",
    "A tangente também pode ser utilizada como função de ativação em redes neurais, e sua derivada é útil para o cálculo do gradiente, que é fundamental em algoritmos de aprendizado de máquina, como o backpropagation.\n",
    "\n",
    "Assim como o seno e o cosseno, a tangente também é uma função periódica e contínua em todo o seu domínio. A tangente de um ângulo pode ser negativa, zero ou positiva, dependendo do quadrante em que o ângulo se encontra.\n",
    "\n",
    "A **derivada do seno(x)** é **igual ao consseno(x)**, e o mesmo acontece com o cosseno(x). A **derivada do cosseno(x)** é o **-seno(x)**. Isso acontece porquê\n",
    "\n",
    "<img src=\"./imgs/derivada_seno.png\">\n",
    "<img src=\"./imgs/derivada_cosseno.png\">\n",
    "\n",
    "\n",
    "A derivada é uma medida da taxa de variação de uma função em relação à sua variável independente. No caso das funções trigonométricas, a derivada pode ser entendida de forma intuitiva como a taxa de variação da posição do ponto que se move sobre uma circunferência unitária.\n",
    "\n",
    "Para entender por que a derivada do seno(x) é igual ao cosseno(x), podemos imaginar um ponto P que se move sobre uma circunferência unitária, começando no ponto (1,0) no eixo x. A medida que o ponto P se move no sentido anti-horário, a coordenada y do ponto P aumenta e a coordenada x diminui. A taxa de variação do y em relação a x é exatamente a inclinação da reta tangente ao ponto P na circunferência unitária.\n",
    "\n",
    "A inclinação da reta tangente é igual à coordenada x do ponto P, que é exatamente o cosseno do ângulo x. Portanto, a derivada do seno(x) é igual ao cosseno(x).\n",
    "\n",
    "Da mesma forma, podemos entender por que a derivada do cosseno(x) é o -seno(x). Quando o ponto P se move no sentido anti-horário sobre a circunferência unitária, a coordenada x do ponto P diminui e a coordenada y aumenta. A taxa de variação do x em relação a y é exatamente a inclinação da reta tangente ao ponto P na circunferência unitária.\n",
    "\n",
    "A inclinação da reta tangente é igual à coordenada y do ponto P multiplicada por -1, que é exatamente o -seno do ângulo x. Portanto, a derivada do cosseno(x) é o -seno(x).\n",
    "\n",
    "<img src=\"./imgs/trigonometric_func1.png\">\n",
    "<img src=\"./imgs/trigonometric_func2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2282b631",
   "metadata": {},
   "source": [
    "## Meaning of the Exponential (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72e814",
   "metadata": {},
   "source": [
    "A exponencial (e) é uma constante matemática aproximadamente igual a 2,71828. Ela é uma das funções matemáticas mais importantes e é usada em diversas áreas da ciência, incluindo física, química, engenharia, economia e estatística.\n",
    "\n",
    "A exponencial pode ser definida como a função matemática que descreve o crescimento exponencial de uma quantidade ao longo do tempo.\n",
    "\n",
    "A exponencial também pode ser vista como uma função que cresce muito rapidamente com o aumento de x. Por exemplo, quando x é igual a 1, e^x é aproximadamente igual a 2,71828. Quando x é igual a 2, e^x é aproximadamente igual a 7,389. E quando x é igual a 3, e^x é aproximadamente igual a 20,086. Isso significa que a exponencial cresce muito rapidamente com o aumento de x, e é uma função muito importante em diversos contextos da matemática e da ciência.\n",
    "\n",
    "Uma das aplicações mais comuns é no cálculo de funções de ativação em redes neurais artificiais. A função sigmóide, que tem a forma 1/(1+e^(-x)). Essa função é uma transformação da função exponencial que mapeia qualquer valor real para um valor entre 0 e 1. Ela é usada para \"suavizar\" a saída de cada neurônio, de forma que o resultado final da rede neural seja mais fácil de interpretar.\n",
    "\n",
    "Outra aplicação comum da exponencial em machine learning é na modelagem de processos de crescimento exponencial. Por exemplo, quando estamos modelando o crescimento de uma população, podemos usar uma função exponencial para descrever a taxa de crescimento ao longo do tempo. Essa função pode ser ajustada aos dados reais de uma população usando técnicas de regressão, permitindo que possamos prever o crescimento futuro da população com base nos dados históricos.\n",
    "\n",
    "Além disso, a exponencial também é usada em outras técnicas de machine learning, como o algoritmo de gradient descent, que é usado para ajustar os parâmetros de um modelo de machine learning para que ele se ajuste aos dados de entrada. A exponencial é uma ferramenta matemática muito importante em machine learning, permitindo que os algoritmos sejam ajustados e otimizados para obter os melhores resultados possíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324757c5",
   "metadata": {},
   "source": [
    "## Derivative of e^x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4694f507",
   "metadata": {},
   "source": [
    "A derivada de um exponencial $e^{x}$ é o próprio $e^{x}$.\n",
    "\n",
    "A forma mais intuitiva de entender por que a derivada da exponencial $e^{x}$ é igual a ela mesma, $e^{x}$, é pensar na própria definição da função exponencial. A função exponencial $e^{x}$ representa o **crescimento exponencial de uma quantidade ao longo do tempo**. Isso significa que, quanto maior o valor de $x$, maior será o valor de $e^{x}$.\n",
    "\n",
    "Quando encontramos a derivada da função exponencial $e^{x}$, estamos essencialmente medindo a taxa de crescimento instantânea da função em um ponto específico. Podemos pensar nessa taxa de crescimento como a inclinação da reta tangente à curva da função exponencial em um ponto específico.\n",
    "\n",
    "Ao calcular a derivada da exponencial, estamos basicamente perguntando \"quanto a função está crescendo neste ponto específico?\". E a resposta é que a função está crescendo a uma taxa exatamente igual ao seu próprio valor, ou seja, a inclinação da reta tangente à curva da função exponencial é igual ao valor da função naquele ponto.\n",
    "\n",
    "Isso ocorre porque, comoa função exponencial é definida como a função que eleva a constante matemática e (aproximadamente 2,71828) à potência x. A derivada da função exponencial $e^{x}$ é encontrada utilizando a regra da cadeia, ou seja, uma fórmula para encontrar a derivada de uma função composta, como é o caso da exponencial.\n",
    "\n",
    "Seja $f(x) = e^{x}$, então:\n",
    "\n",
    "$$ f'(x) = (e^{x})' = e^{x} * (x)' = e^x * 1 = e^{x} $$\n",
    "\n",
    "Portanto, a derivada da exponencial e^x é igual a ela mesma, e^x. Isso significa que a inclinação da reta tangente à curva da função exponencial é igual ao valor da função naquele ponto específico.\n",
    "\n",
    "Dessa forma, podemos entender intuitivamente que a taxa de crescimento instantânea da função exponencial é igual ao seu próprio valor, ou seja, a função exponencial cresce proporcionalmente à sua magnitude em cada ponto, resultando na inclinação da reta tangente igual ao valor da função naquele ponto.\n",
    "\n",
    "<img src=\"./imgs/derivative_e.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a3d68",
   "metadata": {},
   "source": [
    "## Meaning of the Log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2e893f",
   "metadata": {},
   "source": [
    "O logaritmo é uma ferramenta matemática usada para simplificar cálculos com números muito grandes ou muito pequenos. De forma intuitiva, o logaritmo é o expoente a que uma base deve ser elevada para produzir um determinado número. Por exemplo, se tivermos a equação $10^{3} = 1000$, podemos reescrevê-la como log base $10$ de $1000 = 3$. Isso significa que o logaritmo de $1000$ na base $10$ é igual a $3$. Isso porquê $10^{3} = 1000$. **O logaritmo nos diz quantas vezes a base deve ser multiplicada por si mesma para produzir um determinado número**.\n",
    "\n",
    "O logaritmo é usado em Machine Learning (ML) em diversas situações, mas uma das mais comuns é na normalização de dados. Em muitos casos, os dados usados em ML podem variar significativamente em magnitude e escala, o que pode dificultar a análise e a comparação entre eles. Ao aplicar uma transformação logarítmica nos dados, é possível reduzir a variação de escala e trazer os dados para uma faixa mais próxima. Isso pode facilitar a análise e melhorar o desempenho dos modelos. Por exemplo, se tivermos dados que variam de 1 a 10.000, aplicar uma transformação logarítmica nesses dados fará com que eles variem de 0 a 4 (log base 10 de 1 é 0 e de 10.000 é 4). Dessa forma, podemos trabalhar com os dados normalizados e compará-los de maneira mais adequada.\n",
    "\n",
    "Agora, a **derivada de um logaritmo** é utilizado especialmente na otimização de funções de custo (ou loss functions) em algoritmos de aprendizado de máquina em algoritmos como regressão logística, para modelar as relações não lineares entre as variáveis independentes e dependentes.\n",
    "\n",
    "Em muitos casos, o objetivo do aprendizado de máquina é encontrar um conjunto de parâmetros de modelo que minimizem a função de custo. A **derivada do logaritmo** é usada para encontrar o mínimo dessa função de custo.\n",
    "\n",
    "Por exemplo, em regressão logística, a função de custo é dada pela entropia cruzada (cross-entropy), que é uma função logarítmica. A **derivada dessa função** em relação aos parâmetros do modelo é usada no algoritmo de otimização para **atualizar os pesos** do modelo em cada etapa de treinamento. Isso ajuda a ajustar os pesos do modelo de forma a minimizar o erro e melhorar o desempenho do modelo.\n",
    "\n",
    "Além disso, a derivada do logaritmo é usada em outras técnicas de aprendizado de máquina, como na regularização L1 e L2, que são usadas para evitar o overfitting e aumentar a generalização do modelo. Nesse caso, a derivada é usada para calcular a penalidade adicionada à função de custo, que ajuda a evitar pesos muito grandes no modelo.\n",
    "\n",
    "A **derivada do logaritmo** é uma ferramenta essencial em Machine Learning, sendo usada para encontrar os mínimos das funções de custo e para implementar técnicas de regularização que ajudam a evitar o overfitting e aumentar a generalização do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e45c370",
   "metadata": {},
   "source": [
    "## Derivative of log(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ba98c",
   "metadata": {},
   "source": [
    "**A função logarítmica inversa é a exponencial, e vice-versa**. Isso significa que a derivada do logaritmo natural (ou neperiano) pode ser encontrada utilizando a regra da cadeia e as propriedades da exponencial.\n",
    "\n",
    "Uma maneira intuitiva de entender por que a derivada do logaritmo natural de uma função $y$ é igual a $1/y$ é pensar na relação entre logaritmos e exponenciais.\n",
    "\n",
    "O logaritmo de uma função y é o expoente ao qual a base e (aproximadamente 2,71828) deve ser elevada para obter $y$. Por exemplo, se $y = e^{3}$, então o logaritmo natural de $y$ é $log(y) = ln(e^{3}) = 3$.\n",
    "\n",
    "A derivada do logaritmo natural de uma função $y$ é a taxa de variação instantânea do logaritmo em relação a $y$. Podemos interpretar essa taxa de variação como a inclinação da reta tangente à curva do logaritmo em um ponto específico.\n",
    "\n",
    "Ao calcular a derivada do logaritmo natural, estamos essencialmente perguntando \"quanto o logaritmo está mudando neste ponto específico da curva?\". E a resposta é que o logaritmo natural está mudando a uma taxa proporcional ao inverso do valor da função y naquele ponto. Em outras palavras, quanto menor o valor de $y$, mais rápido o logaritmo natural muda, e quanto maior o valor de y, mais devagar ele muda.\n",
    "\n",
    "Isso ocorre porque a relação entre logaritmos e exponenciais nos diz que a derivada do logaritmo natural é o inverso da função exponencial. Ou seja, a derivada do logaritmo natural é igual a $1/y$, porque a derivada da exponencial $e^{x}$ é igual a ela mesma, $e^{x}$.\n",
    "\n",
    "Dessa forma, podemos entender intuitivamente que a taxa de variação do logaritmo natural é inversamente proporcional ao valor da função y naquele ponto, porque a relação entre logaritmos e exponenciais nos diz que as funções logarítmicas e exponenciais são inversas uma da outra.\n",
    "\n",
    "<img src=\"./imgs/derivative_log.png\">\n",
    "\n",
    "A derivada do logaritmo natural é uma ferramenta importante em machine learning para otimizar modelos de aprendizado de máquina e para calcular probabilidades de eventos.\n",
    "\n",
    "Em machine learning, a otimização de modelos é frequentemente realizada usando técnicas de gradiente descendente, que envolvem a atualização iterativa dos parâmetros do modelo com base na taxa de variação da função de custo em relação a esses parâmetros. A função de custo é tipicamente uma função de log-verossimilhança, que é uma função logarítmica da probabilidade de observar os dados, dadas as configurações dos parâmetros do modelo.\n",
    "\n",
    "A derivada do logaritmo natural é útil para calcular a taxa de variação da função de custo em relação aos parâmetros do modelo. Como a função de custo é uma função logarítmica da probabilidade, a derivada do logaritmo natural da probabilidade é igual à derivada da função de custo. Portanto, é comum calcular a derivada do logaritmo natural da probabilidade para atualizar os parâmetros do modelo usando o gradiente descendente.\n",
    "\n",
    "Além disso, a função de custo é frequentemente modelada como uma distribuição de probabilidade, como a distribuição normal ou a distribuição binomial. A derivada do logaritmo natural da distribuição de probabilidade é usada para calcular a probabilidade de observar os dados, dadas as configurações dos parâmetros do modelo. Isso é importante em muitas aplicações de machine learning, como a classificação de imagens, a detecção de fraudes e a recomendação de produtos, onde o objetivo é maximizar a probabilidade de obter resultados precisos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a93f39",
   "metadata": {},
   "source": [
    "## Non Differenciable Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcff15b",
   "metadata": {},
   "source": [
    "**Non Differenciable Functions** são funções que não possuem derivada em pelo menos um ponto de seu domínio. Em outras palavras, a inclinação da reta tangente à curva da função não pode ser determinada em pelo menos um ponto.\n",
    "\n",
    "As funções não diferenciáveis podem apresentar diferentes tipos de descontinuidades, como descontinuidades de salto ou descontinuidades infinitas. Essas descontinuidades podem ocorrer em pontos isolados ou em intervalos inteiros do domínio da função.\n",
    "\n",
    "Algumas funções famosas que não são diferenciáveis em pontos específicos incluem a função valor absoluto $(abs(x))$ em $x = 0$ e a função módulo $(|x|)$ em $x = 0$. Essas funções apresentam uma descontinuidade de salto em x=0 e, portanto, não são diferenciáveis nesse ponto.\n",
    "\n",
    "Outras funções famosas que não são diferenciáveis em intervalos inteiros do domínio incluem a função valor absoluto $(abs(x))$ em $x < 0$ e a função chão $(floor(x))$ em todo o seu domínio. Essas funções apresentam descontinuidades infinitas em seus intervalos de não-diferenciabilidade.\n",
    "\n",
    "<img src=\"./imgs/non_diff_funcs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179dfea6",
   "metadata": {},
   "source": [
    "## Properties of Derivative: Multiplication by Scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d92bd",
   "metadata": {},
   "source": [
    "A multiplicação de derivadas por escalares é uma propriedade afirma que, **para qualquer função f(x) e qualquer constante c, a derivada de c*f(x) é igual a c vezes a derivada de f(x)**.\n",
    "\n",
    "Podemos entender a multiplicação de derivadas por escalares usando o exemplo da multiplicação por 2, **mas ela se aplica a qualquer constante e a qualquer curva**.\n",
    "\n",
    "Suponha que temos uma função f(x) que representa a posição de um objeto em relação ao tempo e queremos encontrar a velocidade do objeto. A velocidade é dada pela derivada da posição em relação ao tempo.\n",
    "\n",
    "Agora, suponha que o objeto esteja se movendo duas vezes mais rápido. Isso significa que a posição do objeto em relação ao tempo foi multiplicada por 2. Se queremos encontrar a nova velocidade do objeto, precisamos multiplicar a derivada da posição por 2, pois a velocidade também foi multiplicada por 2.\n",
    "\n",
    "Por exemplo, se a função f(x) = x^2 representa a posição do objeto em relação ao tempo, sua derivada f'(x) = 2x representa a velocidade do objeto. Se multiplicarmos a função f(x) por 2, obtemos a função g(x) = 2x^2. A derivada de g(x) é dada por g'(x) = 4x, que é 2 vezes maior que f'(x). Isso ocorre porque, ao multiplicar f(x) por 2, estamos multiplicando cada valor de f'(x) por 2.\n",
    "\n",
    "<img src=\"./imgs/propertie_mult1.png\"> \n",
    "<img src=\"./imgs/propertie_mult2.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01250661",
   "metadata": {},
   "source": [
    "## Properties of Derivative: The sum rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47617dd8",
   "metadata": {},
   "source": [
    "A regra da soma de derivadas é uma propriedade do cálculo diferencial que nos permite calcular a derivada de uma soma de funções. Essa regra afirma que, para duas funções $f(x)$ e $g(x)$, a derivada da sua soma $f(x) + g(x)$ é igual à soma das derivadas de cada função $f'(x) + g'(x)$.\n",
    "\n",
    "Matematicamente, podemos escrever a regra da soma de derivadas da seguinte forma:\n",
    "\n",
    "$(f(x) + g(x))' = f'(x) + g'(x)$\n",
    "\n",
    "Podemos entender essa regra de forma intuitiva pensando na derivada como a taxa de variação instantânea de uma função. Se estamos somando duas funções $f(x)$ e $g(x)$, a taxa de variação instantânea da soma é igual à soma das taxas de variação instantâneas de cada função individualmente.\n",
    "\n",
    "Por exemplo, suponha que temos as funções $f(x) = x^2$ e $g(x) = 3x$. Queremos encontrar a derivada da sua soma, $f(x) + g(x)$. Podemos aplicar a regra da soma de derivadas da seguinte forma:\n",
    "\n",
    "$$(f(x) + g(x))' = f'(x) + g'(x)$$\n",
    "\n",
    "Sabemos que a derivada de $f(x)$ é $f'(x) = 2x$, e a derivada de $g(x)$ é $g'(x) = 3$. Portanto, podemos substituir $f'(x)$ e g'(x) na equação acima para obter:\n",
    "\n",
    "$$(f(x) + g(x))' = 2x + 3$$\n",
    "\n",
    "Isso significa que a derivada da soma das funções $f(x)$ e $g(x)$ é igual a $2x + 3$. Essa é a taxa de variação instantânea da soma das funções $f(x)$ e $g(x)$ em qualquer ponto $x$.\n",
    "\n",
    "<img src=\"./imgs/propertie_sum_rule1.png\">\n",
    "<img src=\"./imgs/propertie_sum_rule2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e9b22",
   "metadata": {},
   "source": [
    "## Properties of Derivative: The product rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6fefae",
   "metadata": {},
   "source": [
    "A **product rule** é bem semelhante a **sum rule**, mas com um diferencial.\n",
    "\n",
    "$$ f = gh $$\n",
    "\n",
    "$$ f' = g'h + gh'$$\n",
    "\n",
    "<img src=\"./imgs/product_rule.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241010e2",
   "metadata": {},
   "source": [
    "## Properties of Derivative: The chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a352f59",
   "metadata": {},
   "source": [
    "A regra da cadeia (chain rule, em inglês) é uma regra do cálculo que nos permite calcular a derivada de uma função composta (\"uma dentro da outra\"). Funções compostas são aquelas que consistem de uma função \"externa\" e uma função \"interna\" aplicada a ela. A regra da cadeia nos diz como as derivadas da função interna e externa se combinam para dar a derivada da função composta. **O resultado de uma função composta é o produto das derivadas internas**.\n",
    "\n",
    "De forma intuitiva, podemos pensar na regra da cadeia como um efeito dominó. Quando aplicamos uma função composta a um valor x, primeiro aplicamos a função interna ao valor x, obtendo um novo valor. Em seguida, aplicamos a função externa a esse novo valor, obtendo o resultado final da função composta.\n",
    "\n",
    "A regra da cadeia nos diz que, para calcular a derivada da função composta, precisamos **multiplicar (produto) a derivada da função interna pela derivada da função externa aplicada à função interna**. Ou seja, se f(x) é a função composta de u(v(x)), então a derivada de f(x) é dada por:\n",
    "\n",
    "$$\\frac{d}{dt}f(g(h(t))) = \\frac{df}{dg}\\cdot\\frac{dg}{dh}\\cdot\\frac{dh}{dt}$$\n",
    "\n",
    "<img src=\"./imgs/idea_chain_rule.png\">\n",
    "\n",
    "<img src=\"./imgs/chain_rule.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8f64ca",
   "metadata": {},
   "source": [
    "## Meaning of Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca5adda",
   "metadata": {},
   "source": [
    "Otimização é um ramo da matemática que lida com a **maximização ou minimização de uma função** em relação a suas variáveis. Em outras palavras, o objetivo da otimização é **encontrar a melhor solução possível** para um problema, geralmente sujeito a uma série de restrições.\n",
    "\n",
    "Em muitos casos, é possível usar a derivada de uma função para encontrar o mínimo ou máximo global dessa função. Esse processo é conhecido como otimização diferenciável. Quando uma função é diferenciável, podemos encontrar seus pontos críticos calculando a derivada da função e encontrando os pontos onde a derivada é igual a zero. Em seguida, podemos avaliar a função em cada um desses pontos críticos para determinar o mínimo ou máximo global.\n",
    "\n",
    "Em machine learning, a otimização é frequentemente usada para **ajustar os parâmetros de um modelo de aprendizado de máquina para minimizar o erro em um conjunto de dados de treinamento**. A otimização é realizada por meio de um algoritmo de otimização, que geralmente **usa a derivada da função de perda em relação aos parâmetros** do modelo para atualizar esses parâmetros em cada etapa do processo de treinamento.\n",
    "\n",
    "Um exemplo comum de algoritmo de otimização usado em machine learning é o gradiente descendente. O gradiente descendente é um algoritmo iterativo que usa a derivada da função de perda em relação aos parâmetros do modelo para atualizar esses parâmetros na direção do mínimo local da função de perda. O algoritmo continua a iterar até que a função de perda seja minimizada ou até que uma condição de parada seja atendida.\n",
    "\n",
    "Em resumo, a otimização é um importante conceito matemático que é amplamente utilizado em machine learning para ajustar os parâmetros de um modelo de aprendizado de máquina para minimizar o erro em um conjunto de dados de treinamento. A derivada é frequentemente usada para encontrar o mínimo ou máximo global de uma função, o que é útil para ajustar os parâmetros do modelo de forma eficiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f437b2cd",
   "metadata": {},
   "source": [
    "## Optimization of Squared Loss - Powerline Problem\n",
    "\n",
    "Esse problema consiste em mover a casa para o mais próximo possível da rede elétrica, para diminuir o custo de se ter energia, de se concetar à rede elétrica. Esse custo se dá pelo tamanho do cabeamento e quanto mais longe da rede, maior é o custo. Esse custo é similar ao **erro quadrado (squared error)**.\n",
    "\n",
    "Diante de um número X de powerlines, precisamos construir a casa no local com menor custo para acessar a rede elétrica, ou seja, menor squared error.\n",
    "\n",
    "<img src=\"./imgs/powerline_problem.png\">\n",
    "\n",
    "**The One Powerline:**\n",
    "\n",
    "Possuindo somente um ponto de distribuição, o menor custo, ou seja, o menor squared error é o local mais próximo do ponto de distribuição.\n",
    "\n",
    "<img src=\"./imgs/powerline_problem1.png\">\n",
    "\n",
    "**The Two Powerline**:\n",
    "\n",
    "Agora, possuindo dois pontos de distribuição, precisamos encontrar a localização que minimiza o custo considerando esses dois pontos, diminuindo o custo de se conectar em ambos. Ou seja, econtrar o x, a distância da casa até a origem, que minimize a função de custo $(x - a)^{2}+(x - b)^{2}$. **Precisamos encotrar o ponto em que a sua derivada é zero**\n",
    "\n",
    "<img src=\"./imgs/powerline_problem2.png\">\n",
    "<img src=\"./imgs/powerline_problem22.png\">\n",
    "\n",
    "**The Three Powerline:**\n",
    "Inserindo mais um ponto de distribuição, temos o mesmo problema mais com mais uma variável. Tendo o mesmo objetivo **minimizar o custo de se conectar com todos os pontos de distribuição**.\n",
    "\n",
    "<img src=\"./imgs/powerline_problem3.png\">\n",
    "<img src=\"./imgs/powerline_problem33.png\">\n",
    "<img src=\"./imgs/powerline_problem333.png\">\n",
    "\n",
    "**The Squared Loss:**\n",
    "Podemos generalizar a função de custo para n número de casas, ou, n número de variáveis. \n",
    "\n",
    "$$minimize(x - a1){2} + (x - a2){2} + ... + (x - an){2}$$ \n",
    "\n",
    "$$Solution: x = \\frac{a1 + a2 + ... + an}{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426bac8",
   "metadata": {},
   "source": [
    "## Optmization of log-loss (rever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c38eb4",
   "metadata": {},
   "source": [
    "A função de custo log-loss é uma medida comumente usada em problemas de classificação em machine learning. Ela é usada para medir o quão bem o modelo está prevendo as classes corretas para cada exemplo de treinamento.\n",
    "\n",
    "De forma intuitiva, a otimização da função de custo log-loss envolve a busca pelo conjunto de parâmetros do modelo que minimizam a diferença entre as classes reais e as classes previstas pelo modelo. Isso é feito encontrando o ponto mais baixo na curva da função de custo log-loss.\n",
    "\n",
    "Para entender melhor, imagine que estamos construindo um modelo de classificação binária para prever se um determinado e-mail é spam ou não. Para isso, usamos uma técnica de aprendizado supervisionado e treinamos nosso modelo com um conjunto de exemplos de e-mails rotulados como spam ou não spam.\n",
    "\n",
    "Ao avaliar nosso modelo, usamos a função de custo log-loss para medir o quão bem ele está prevendo as classes corretas para cada exemplo de treinamento. A função de custo log-loss é uma medida que penaliza o modelo quando ele faz previsões incorretas, dando maior peso para as previsões erradas.\n",
    "\n",
    "A otimização da função de custo log-loss envolve encontrar os parâmetros do modelo que minimizam essa penalidade global, ou seja, aqueles que fazem com que a função de custo log-loss atinja o ponto mais baixo possível. Isso é feito usando técnicas de otimização, como o gradiente descendente, que usam a derivada da função de custo log-loss em relação aos parâmetros do modelo para ajustar esses parâmetros em cada etapa do processo de treinamento.\n",
    "\n",
    "Em resumo, a otimização da função de custo log-loss envolve encontrar o conjunto de parâmetros do modelo que minimizam a diferença entre as classes reais e as classes previstas pelo modelo. Isso é feito usando técnicas de otimização que usam a derivada da função de custo log-loss em relação aos parâmetros do modelo para ajustar esses parâmetros e encontrar o ponto mais baixo da curva da função de custo log-loss.\n",
    "\n",
    "Suponha que estamos construindo um modelo de classificação de imagens que pode distinguir entre imagens de gatos e imagens de cachorros. Para treinar esse modelo, usamos um conjunto de dados rotulados com exemplos de imagens de gatos e cachorros.\n",
    "\n",
    "Depois de criar um modelo inicial, precisamos ajustar seus parâmetros para que ele seja capaz de prever corretamente se uma imagem é de um gato ou de um cachorro. Para fazer isso, usamos uma função de custo, como a log-loss, que mede o quão bem o modelo está fazendo essas previsões.\n",
    "\n",
    "A log-loss penaliza o modelo quando ele faz previsões incorretas, dando maior peso para as previsões erradas. Queremos minimizar a log-loss, ou seja, queremos que o modelo faça previsões cada vez melhores e mais precisas.\n",
    "\n",
    "Para ajustar os parâmetros do modelo e minimizar a log-loss, usamos uma técnica de otimização, como o gradiente descendente. O gradiente descendente usa a derivada da função de custo log-loss em relação aos parâmetros do modelo para ajustar esses parâmetros em cada etapa do processo de treinamento.\n",
    "\n",
    "Por exemplo, suponha que a log-loss atual do modelo seja 0,6 e queremos ajustar os parâmetros para reduzi-la. O gradiente descendente calcula a derivada da log-loss em relação aos parâmetros e, em seguida, ajusta os parâmetros em uma direção que reduz a log-loss.\n",
    "\n",
    "Isso é feito iterativamente até que o modelo atinja o ponto mais baixo da curva da log-loss, onde a log-loss é mínima e o modelo é capaz de fazer previsões muito precisas e confiáveis sobre se uma imagem é de um gato ou de um cachorro.\n",
    "\n",
    "Em resumo, a otimização da função de custo log-loss é usada para ajustar os parâmetros de um modelo de forma a minimizar a diferença entre as classes reais e as classes previstas pelo modelo. Isso é feito usando técnicas de otimização, como o gradiente descendente, que usam a derivada da função de custo log-loss em relação aos parâmetros do modelo para ajustar esses parâmetros e encontrar o ponto mais baixo da curva da função de custo log-loss.\n",
    "\n",
    "A função de custo log-loss é dada por:\n",
    "\n",
    "$$ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^m y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)})) $$\n",
    "\n",
    "onde $\\theta$ são os parâmetros do modelo, $m$ é o número de exemplos de treinamento, $y^{(i)}$ é a classe real do exemplo de treinamento $i$, $x^{(i)}$ é o vetor de features do exemplo de treinamento $i$, e $h_\\theta(x^{(i)})$ é a classe prevista pelo modelo para o exemplo de treinamento $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea77b31e",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52e818b",
   "metadata": {},
   "source": [
    "## Tangent Planes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c6f0b",
   "metadata": {},
   "source": [
    "O Tangent Plane (plano tangente) é um conceito matemático que se refere a um plano que toca uma superfície em um determinado ponto, e tem a mesma inclinação da superfície nesse ponto. Em outras palavras, o Tangent Plane é o plano que melhor aproxima a superfície em um ponto específico, e é usado para descrever a geometria local da superfície nesse ponto.\n",
    "\n",
    "O Tangent Plane é usado em machine learning para realizar otimizações locais em funções de **várias variáveis** (até agora vimos somente com uma). Em muitos algoritmos de otimização, é necessário calcular a inclinação ou a derivada de uma função em um ponto específico, e o Tangent Plane pode ser usado para aproximar a função localmente em torno desse ponto.\n",
    "\n",
    "Por exemplo, em redes neurais, é comum usar o Tangent Plane para ajustar os pesos da rede durante o treinamento. A cada iteração, a inclinação da função de perda em relação aos pesos é calculada em um ponto específico, e o Tangent Plane é usado para aproximar a função localmente nesse ponto. Isso permite ajustar os pesos de forma a minimizar a função de perda em relação aos dados de treinamento.\n",
    "\n",
    "<img src=\"./imgs/tangent_plane1.png\">\n",
    "<img src=\"./imgs/tangent_plane2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b5c93",
   "metadata": {},
   "source": [
    "## Partial Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccd330",
   "metadata": {},
   "source": [
    "Derivadas parciais são um conceito matemático usado para medir **como uma função muda em relação a uma de suas variáveis independentes, mantendo as outras variáveis constantes**. Em outras palavras, as derivadas parciais são as taxas de variação locais de uma função de várias variáveis em relação a cada uma de suas variáveis independentes, uma de cada vez.\n",
    "\n",
    "Por exemplo, considere uma função f(x,y) que representa a altura de uma montanha em diferentes pontos (x,y) no mapa. Para encontrar a inclinação da montanha em um ponto específico (x0,y0), podemos calcular as derivadas parciais de f em relação a x e y no ponto (x0,y0). A derivada parcial em relação a x nos dá a inclinação da montanha na direção x, mantendo y constante, enquanto a derivada parcial em relação a y nos dá a inclinação da montanha na direção y, mantendo x constante.\n",
    "\n",
    "As derivadas parciais são usadas em machine learning para ajustar os parâmetros de um modelo de forma a minimizar a função de perda em relação aos dados de treinamento. Isso é feito por meio de algoritmos de otimização, como o Gradiente Descendente, que usam as derivadas parciais da função de perda em relação a cada um dos parâmetros do modelo para atualizar seus valores em cada iteração.\n",
    "\n",
    "Por exemplo, considere uma rede neural que possui pesos (parâmetros) w1, w2, ..., wn. Durante o treinamento, a rede neural recebe um conjunto de dados de treinamento e tenta ajustar seus pesos para minimizar a função de perda em relação aos dados de treinamento. Para fazer isso, é necessário calcular a derivada parcial da função de perda em relação a cada um dos pesos.\n",
    "\n",
    "O Gradiente Descendente usa essas derivadas parciais para atualizar os valores dos pesos em cada iteração, de forma a minimizar a função de perda. A ideia é seguir na direção oposta ao gradiente da função de perda, pois essa direção corresponde à maior taxa de variação negativa da função. Dessa forma, os valores dos pesos são atualizados iterativamente até que a função de perda seja minimizada.\n",
    "\n",
    "As derivadas parciais também são usadas em outras áreas do machine learning, como regressão linear, análise discriminante e classificação de imagens, para citar alguns exemplos. Em geral, as derivadas parciais são usadas para medir a sensibilidade de uma função em relação a seus parâmetros, permitindo ajustá-los de forma a melhorar o desempenho do modelo.\n",
    "\n",
    "<img src=\"./imgs/derivada_parcial.png\">\n",
    "\n",
    "Podemos calcular a derivada parcial para cada variável em relação as demais, nesse caso, temos $x^{2} + y^{2}$. Tratando y como uma constante, a darivada parcial é 2x. Mas caso x seja tratado como uma constante, o resultado será 2y.\n",
    "\n",
    "$$f(x, y)$$\n",
    "\n",
    "Derivada parcial de f em relação ao x:\n",
    "$$fx = \\frac{df}{dx}$$\n",
    "\n",
    "Derivada parcial de f em relação ao y:\n",
    "$$fx = \\frac{df}{dy}$$\n",
    "\n",
    "Para chegarmos ao resultado precisamos seguir dois passos;\n",
    "1. Tratar as demais variáveis como constante.\n",
    "2. Seguir as regras de diferenciação (cálculo de derivada)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc76196",
   "metadata": {},
   "source": [
    "## Gradientes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5da6669",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-20T22:36:19.576269Z",
     "start_time": "2023-03-20T22:36:19.573997Z"
    }
   },
   "source": [
    "**Um gradiente é um vetor que contêm as derivadas parciais**. $$f(x, y) = x^{2} + y^{2}$$\n",
    "\n",
    "O Gradiente de $f(x, y)$ é:\n",
    "\n",
    "$$\\nabla f = \\begin{bmatrix} 2x \\\\ 2y \\end{bmatrix}$$\n",
    "\n",
    "<img src=\"./imgs/gradiente1.png\">\n",
    "\n",
    "O plano tangente (laranja) descreve a inclinação das duas linhas que formam o plano tangente. \n",
    "\n",
    "<img src=\"./imgs/gradiente2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd104da",
   "metadata": {},
   "source": [
    "## Gradients and Maxima/Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91b1f5",
   "metadata": {},
   "source": [
    "O gradiente é util pra minimizar funções com mais de uma variável, da mesma forma que a derivada é util para minimizar a função com uma variável.\n",
    "\n",
    "Para encontrar o minimums e o máximums precisamos encontrar os pontos das derivadas parciais, o gradiente, que minimizem o valor de ambos. Ou maximizem o valor de ambos. Pra isso, precisamos _settar_ as derivadas parciais para zero, e resolver o sistema de equações (nesse caso).\n",
    "\n",
    "<img src=\"./imgs/gradiente_minmax.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ad139a",
   "metadata": {},
   "source": [
    "## Optimization with Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc9dd0f",
   "metadata": {},
   "source": [
    "Numa sauna, a **pessoa A** deseja encontrar o ponto da sala em que faz menos calor. Como ela pode encotrar ? Através do gradiente descendente!\n",
    "\n",
    "Para cada pontonto xy, em que x e y são duas coordenadas, aqui, a temperatura é o **valor da função naquele ponto**, representado pela altura T(ºC). \n",
    "\n",
    "Aqui o objetivo é **encontrar o ponto mais frio ou menos quente da sala**, ou seja minimizar a função de custo em relação a temperatura. Precisamos chear ao ponto em que o gradiente que minimiza o valor das duas derivadas parciais, em relação a x e em relação a y. Encontrarmos, através do gradiente descendente, o ponto em que a inclinação é zero. Nesse ponto, o plano tangente é paralelo ao \"chão\". Aqui ponto, para qualquer direção que nos movermos nesta sala, a temperatura irá aumentar.\n",
    "\n",
    "<img src=\"./imgs/gradiente_sauna1.png\">\n",
    "\n",
    "Agora, verificando matemáticamente:\n",
    "\n",
    "- Primeiro, precisamos encotrar as derivadas parciais em relação a x e a y; Precisamos tratar y como uma constante para encontrar a derivada parcial em relação ao x, e depois o x como uma contante para encontrar a derivada parcial em relação ao y. \n",
    "- Depois, precisamos igualar ambos a zero, para contrar o ponto que minimiza a função; como o $\\frac{df}{dx}$ e $\\frac{df}{dy}$ é um produto de vários elementos e precisamos que o resultado seja zero, basta substituir uma das variáveis por zero para que o resultado das multiplicações seja zero, para que cheguemos ao resultado!\n",
    "\n",
    "<img src=\"./imgs/gradiente_sauna2.png\">\n",
    "\n",
    "Agora, basta verificarmos em que ponto no plano as coordenadas x e y minimizam a temperatura.\n",
    "\n",
    "<img src=\"./imgs/gradiente_sauna3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00468684",
   "metadata": {},
   "source": [
    "## Optimization with Gradients Using Analytical Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc758ea",
   "metadata": {},
   "source": [
    "Voltando àquele mesmo problema de rede elétrica, agora, precisamos encontrar o lugar ótimo para que a linha que se conecta aos pontos de distribuição tenha o menor custo. O objetivo é **minimizar a soma dos quadrados** (custo).\n",
    "\n",
    "<img src=\"./imgs/linear_reg1.png\">\n",
    "\n",
    "Para calcular a função que queremos minimizar, precisamos encontrar os pontos em que a linha da regressão se conecta aos pontos de distribuição. Por exemplo, o ponto azul, se conecta à linha no ponto $(1, m+b)$, porquê se $x = 1$, então $mx + b$ é igual a $m + b$. **O ponto em que se conecta ao ponto de distribuição é m+b, ou seja, 2**. Assim y = mx + b é o mesmo que $2 = 1 \\cdot 1 + 1$. Mas a distância é $(m+b -2)$ e o custo é $(m+b -2)^{2}$. E o mesmo segue para os outros dois pontos laranja e verde. Assim, chegamos nas três funções de custo.\n",
    "\n",
    "<img src=\"./imgs/linear_reg2.png\">\n",
    "\n",
    "Para minimizar a função de custo $E(m, b)$, precisamos encontrar as derivadas parciais em relação ao m e ao b, igualando ambos a zero e resolvendo para m e b.\n",
    "\n",
    "<img src=\"./imgs/linear_reg3.png\">\n",
    "\n",
    "Para resolvermos, precisamos usar álgebra linear para resolver esse **sistema de equações**.\n",
    "\n",
    "Para resolver a segunda equação, **multiplicamos os valores por 2**, subtraísmos a primeira equação por ela, chegando em $4m - 2 = 0$, depois chegamos a $m = \\frac{2}{4} = 0.5$. Com esse resultado de $m = 0.5$, colocamos o valor de m na segunda equação e resolvemos para encontrar o valor de b e resolver a primeira equação.\n",
    "\n",
    "<img src=\"./imgs/linear_reg4.png\">\n",
    "<img src=\"./imgs/linear_reg5.png\">\n",
    "\n",
    "**Esses dois valores de m e b, são os valores que fazem as duas derivadas parciais serem zero.**\n",
    "\n",
    "Agora, verificando no gráfico\n",
    "<img src=\"./imgs/linear_reg6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fc8fe4",
   "metadata": {},
   "source": [
    "## Analytical Gradient is same Linear Regression ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b851efd",
   "metadata": {},
   "source": [
    "**Não**, gradiente analítico e regressão linear são conceitos diferentes, embora possam estar relacionados em certos contextos.\n",
    "\n",
    "Gradiente analítico é um método de otimização usado para encontrar os parâmetros ideais de um modelo de aprendizado de máquina. Esse método envolve calcular o gradiente da função de custo (também conhecida como função objetivo) em relação aos parâmetros do modelo e, em seguida, atualizar iterativamente esses parâmetros na direção oposta do gradiente, com o objetivo de minimizar a função de custo.\n",
    "\n",
    "Por outro lado, regressão linear é um tipo de modelo de aprendizado de máquina que visa modelar a relação entre uma variável dependente (ou variável de resposta) e uma ou mais variáveis independentes (ou variáveis preditoras) usando uma função linear. A regressão linear pode ser usada para fins de previsão ou para entender a relação entre as variáveis.\n",
    "\n",
    "Embora o gradiente analítico possa ser usado para encontrar os parâmetros ideais de um modelo de regressão linear, ele também pode ser usado para otimizar outros tipos de modelos de aprendizado de máquina, como redes neurais. Portanto, embora os dois conceitos possam estar relacionados em certos contextos, eles não são a mesma coisa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074160df",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40a24406",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-22T23:05:20.875783Z",
     "start_time": "2023-03-22T23:05:20.467161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivada de f em relação a x:  2*x*y + 6*x\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "# Definindo as variáveis x e y\n",
    "x, y = symbols('x y')\n",
    "\n",
    "# Definindo a função f\n",
    "f = x**2*y + 3*x**2\n",
    "\n",
    "# Calculando a derivada parcial em relação a x\n",
    "df_dx = diff(f, x)\n",
    "\n",
    "print(\"Derivada de f em relação a x: \", df_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1b590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1261e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54792290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311c9d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5406fca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c346f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0169cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2dc5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4bb900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39038461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0a9348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b78353b6",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.465px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
